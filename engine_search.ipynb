{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Takriz.csv\", index_col=0)\n",
    "del df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              Corpus             Entity\n0                                                NaN                NaN\n1        Standard join team focus put customer first  Intitule_du_poste\n2  continued success driven high performance culture                NaN\n3  look people collaborative accountable creative...  Intitule_du_poste\n4  offer care culture make real difference every ...  Intitule_du_poste",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Corpus</th>\n      <th>Entity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Standard join team focus put customer first</td>\n      <td>Intitule_du_poste</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>continued success driven high performance culture</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>look people collaborative accountable creative...</td>\n      <td>Intitule_du_poste</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>offer care culture make real difference every ...</td>\n      <td>Intitule_du_poste</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.where(df['Entity'] ==\"Formation\").dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(49, 3)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                Corpus     Entity\n71   believe open communication stress accountabili...  Formation\n83   person interact team Account Management Applic...  Formation\n101  Guiding Principles Qualifications Bachelor hig...  Formation\n204  take complex visual challenge build custom mac...  Formation\n254  team focus Deep Learning software hardware tec...  Formation",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Corpus</th>\n      <th>Entity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>71</th>\n      <td>believe open communication stress accountabili...</td>\n      <td>Formation</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>person interact team Account Management Applic...</td>\n      <td>Formation</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>Guiding Principles Qualifications Bachelor hig...</td>\n      <td>Formation</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>take complex visual challenge build custom mac...</td>\n      <td>Formation</td>\n    </tr>\n    <tr>\n      <th>254</th>\n      <td>team focus Deep Learning software hardware tec...</td>\n      <td>Formation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "competance = {'programming_language': ('JavaScript', 'Html', 'css','PHP','python','sql','Bash','shell','powerShell','Bash/shell/powerShell','Analyst','lead','C','C#','C++','typescript','ruby','go','assembly','swift','scala','Objective-C','Delphi','Ruby','MATLAB','Dart','COBOL','ABAP','Fortran','Lua','Rust','Apex','Kotlin','Clojure','Scheme'), \n",
    "          'framework': ('React', 'ASP.NET MVC','.NET',' Angular','Ruby on Rails','AngularJS','Angular','Vue.js','Vue','Django','Laravel','Spring','ASP.NET','Express','Flask','Symfony','Meteor','QT','CodeIgniter','JSF','Ember.js','Ember','.NET Core','Google Web Toolkit','CakePHP','Polymer','Play','Sails.js','Zend','JHipster','Tornado','Yii','Phoenix','Sinatra','Grails','Koa','Aurelia','Blade','Svelte','Slim','Vert.x','Phalcon','Vapor','LoopBack','Gin','hapi','OpenUI5','Dropwizard','Elm','Bottle','Wicket','AIOHTTP','Feathers','Dojo','beego','Pyramid','Nancy','Vaadin','web.py','Mithril','Adonis','FastAPI','Struts','web2py','Lucky','Revel','Kohana','Riot.js','Durandal','Ktor','Perfec','Sanic','Yesod','Compojure','CherryPy','Mojolicious','Ring','Falcon','SilverStripe','Sapphire','Martin','Fat-Free','Framework','Tapestry','Iris','Rocket','Actix','web','Marko','FuelPHP','Restlet','Kitura','Scalatra','Ninja','Sailo','SproutCore','Hanami','(fab)','Inferno','Lift','Lithium','Buffalo','Cappuccino','ZK','Flight','servant','Dancer2','Wt','Ratpack','Grok','Zop','CompoundJS','Catalyst','Kemal','TurboGears','Noir','Nitrogen','Amber','Mojarra','seaside\t','Stripes','Masonite','Camping','Cuba','Happstack','Zotonic','Hyperstack','Merb','Cairngorm','Rapidoid','PureMVC','kotlinx.html','Horde','Ramaze','Cocoon','Mate','Orbit','Tipfy','Spider-Gazelle','Maso','Swiz','Prado','Aura','Picard','Erlang','Web','JavaScriptMVC','Vanilla','Solar','Qcodo','Rum','Cutelyst','Apache','Click','QCubed','Xitrum','Swiftlet','Snap','RestfulX','Conjure','Hexagon','Moustache','Eliom','KumbiaPHP',',For','Application','echo','Circumflex','node-route','Sin','ErlyWeb','MFlow','Portofino',',Nerve','Lemmachine','Konstrukt','Wee','Hemlock','nodemachine','simplex','Nitro','Halcyon'),\n",
    "          'no sql': ('mongoDB', 'mongo', 'neo4j','cassandra'),\n",
    "          'database_technologies': ('Hadoop','Spark','scala','elasticsearch','Kibana','Redis','Solr','Apache solr','Lucene','Grafana','Apache Kafka','Fluentd','RabbitMQ','PostgreSQL','Docker','InfluxDB','dynamodb','CouchDB','MariaDB','HBase','ArangoDB','SQLite','SQL Server','Amazon Relational Database Service','Oracle','Microsoft SQL Server'),\n",
    "          'data_visualization_platform': ('Sisense', 'Looker', 'Zoho Analytics','Reveal','Periscope Data','Tableau','Domo','Microsoft Power BI','Power BI','QlikView','Klipfolio','IBM Watson Analytics','Kibana','plotly','Chartio','Infogram','Highcharts','Visme','Geckoboard'),\n",
    "          'IDE': ('Visual Studio', 'vs code ', 'IntelliJ IDEA','Aptana Studio','PyCharm','PhpStorm','WebStorm','NetBeans','Eclipse','RubyMine','Komodo IDE','code blocks','Xcode','Atom','QT creator','Sublime Text','notepad','Brackets'), \n",
    "          'software': ('excel', 'Microsoft Office', 'outlook','Microsoft word','Adobe','photoshop','illustrator','InDesign','Creative Cloud','After-effects'),\n",
    "          'communication': ('communication skills', 'Communication', 'write in a clear',' written and verbal communication')  \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package reuters to\n[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ASIAN EXPORTERS FEAR DAMAGE FROM USJAPAN RIFT\n  Mounting trade friction between the\n  US And Japan has raised fears among many of Asias exporting\n  nations that the row could inflict farreaching economic\n  damage businessmen and officials said\n      They told Reuter correspondents in Asian capitals a US\n  Move against Japan might boost protectionist sentiment in the\n  US And lead to curbs on American imports of their products\n      But some exporters said that while the conflict would hurt\n  them in the longrun in the shortterm Tokyos loss might be\n  their gain\n      The US Has said it will impose 300 mln dlrs of tariffs on\n  imports of Japanese electronics goods on April 17 in\n  retaliation for Japans alleged failure to stick to a pact not\n  to sell semiconductors on world markets at below cost\n      Unofficial Japanese estimates put the impact of the tariffs\n  at 10 billion dlrs and spokesmen for major electronics firms\n  said they would virtually halt exports of products hit by the\n  new taxes\n      We wouldnt be able to do business said a spokesman for\n  leading Japanese electronics firm Matsushita Electric\n  Industrial Co Ltd ltMCT\n      If the tariffs remain in place for any length of time\n  beyond a few months it will mean the complete erosion of\n  exports of goods subject to tariffs to the US said Tom\n  Murtha a stock analyst at the Tokyo office of broker ltJames\n  Capel and Co\n      In Taiwan businessmen and officials are also worried\n      We are aware of the seriousness of the US Threat against\n  Japan because it serves as a warning to us said a senior\n  Taiwanese trade official who asked not to be named\n      Taiwan had a trade trade surplus of 156 billion dlrs last\n  year 95 pct of it with the US\n      The surplus helped swell Taiwans foreign exchange reserves\n  to 53 billion dlrs among the worlds largest\n      We must quickly open our markets remove trade barriers and\n  cut import tariffs to allow imports of US Products if we\n  want to defuse problems from possible US Retaliation said\n  Paul Sheen chairman of textile exporters ltTaiwan Safe Group\n      A senior official of South Koreas trade promotion\n  association said the trade dispute between the US And Japan\n  might also lead to pressure on South Korea whose chief exports\n  are similar to those of Japan\n      Last year South Korea had a trade surplus of 71 billion\n  dlrs with the US Up from 49 billion dlrs in 1985\n      In Malaysia trade officers and businessmen said tough\n  curbs against Japan might allow hardhit producers of\n  semiconductors in third countries to expand their sales to the\n  US\n      In Hong Kong where newspapers have alleged Japan has been\n  selling belowcost semiconductors some electronics\n  manufacturers share that view But other businessmen said such\n  a shortterm commercial advantage would be outweighed by\n  further US Pressure to block imports\n      That is a very shortterm view said Lawrence Mills\n  directorgeneral of the Federation of Hong Kong Industry\n      If the whole purpose is to prevent imports one day it will\n  be extended to other sources Much more serious for Hong Kong\n  is the disadvantage of action restraining trade he said\n      The US Last year was Hong Kongs biggest export market\n  accounting for over 30 pct of domestically produced exports\n      The Australian government is awaiting the outcome of trade\n  talks between the US And Japan with interest and concern\n  Industry Minister John Button said in Canberra last Friday\n      This kind of deterioration in trade relations between two\n  countries which are major trading partners of ours is a very\n  serious matter Button said\n      He said Australias concerns centred on coal and beef\n  Australias two largest exports to Japan and also significant\n  US Exports to that country\n      Meanwhile USJapanese diplomatic manoeuvres to solve the\n  trade standoff continue\n      Japans ruling Liberal Democratic Party yesterday outlined\n  a package of economic measures to boost the Japanese economy\n      The measures proposed include a large supplementary budget\n  and record public works spending in the first half of the\n  financial year\n      They also call for steppedup spending as an emergency\n  measure to stimulate the economy despite Prime Minister\n  Yasuhiro Nakasones avowed fiscal reform program\n      Deputy US Trade Representative Michael Smith and Makoto\n  Kuroda Japans deputy minister of International Trade and\n  Industry MITI are due to meet in Washington this week in an\n  effort to end the dispute\n  \n\n\n"
    }
   ],
   "source": [
    "exclude = set(string.punctuation)\n",
    "alldocslist = []\n",
    "\n",
    "for index, i in  enumerate(reuters.fileids()):\n",
    "    text = reuters.raw(fileids=[i])\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    alldocslist.append(text)\n",
    "    \n",
    "print(alldocslist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['CHINA', 'DAILY', 'SAYS', 'VERMIN', 'EAT', '712', 'PCT', 'GRAIN', 'STOCKS', 'A', 'survey', 'of', '19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'Chinas', 'grain', 'stocks', 'the', 'China', 'Daily', 'said', 'It', 'also', 'said', 'that', 'each', 'year', '1575', 'mln', 'tonnes', 'or', '25', 'pct', 'of', 'Chinas', 'fruit', 'output', 'are', 'left', 'to', 'rot', 'and', '21', 'mln', 'tonnes', 'or', 'up', 'to', '30', 'pct', 'of', 'its', 'vegetables', 'The', 'paper', 'blamed', 'the', 'waste', 'on', 'inadequate', 'storage', 'and', 'bad', 'preservation', 'methods', 'It', 'said', 'the', 'government', 'had', 'launched', 'a', 'national', 'programme', 'to', 'reduce', 'waste', 'calling', 'for', 'improved', 'technology', 'in', 'storage', 'and', 'preservation', 'and', 'greater', 'production', 'of', 'additives', 'The', 'paper', 'gave', 'no', 'further', 'details']\n"
    }
   ],
   "source": [
    "#tokenize words in all DOCS \n",
    "plot_data = [[]] * len(alldocslist)\n",
    "\n",
    "for doc in alldocslist:\n",
    "    text = doc\n",
    "    tokentext = word_tokenize(text)\n",
    "    plot_data[index].append(tokentext)\n",
    "    \n",
    "print(plot_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['china',\n 'daily',\n 'says',\n 'vermin',\n 'eat',\n '712',\n 'pct',\n 'grain',\n 'stocks',\n 'a']"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "#make all words lower case for all docs \n",
    "for x in range(len(reuters.fileids())):\n",
    "    lowers = [word.lower() for word in plot_data[0][x]]\n",
    "    plot_data[0][x] = lowers\n",
    "\n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['china',\n 'daily',\n 'says',\n 'vermin',\n 'eat',\n '712',\n 'pct',\n 'grain',\n 'stocks',\n 'survey']"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "# remove stop words from all docs \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for x in range(len(reuters.fileids())):\n",
    "    filtered_sentence = [w for w in plot_data[0][x] if not w in stop_words]\n",
    "    plot_data[0][x] = filtered_sentence\n",
    "\n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ltaha',\n 'automot',\n 'technolog',\n 'corp',\n 'year',\n 'net',\n 'shr',\n '43',\n 'ct',\n 'vs']"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "#stem words EXAMPLE (could try others/lemmers )\n",
    "\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = [snowball_stemmer.stem(w) for w in filtered_sentence]\n",
    "stemmed_sentence[0:10]\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = [ porter_stemmer.stem(w) for w in filtered_sentence]\n",
    "stemmed_sentence[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverse index which gives document number for each document and where word appears\n",
    "\n",
    "#first we need to create a list of all words \n",
    "l = plot_data[0]\n",
    "flatten = [item for sublist in l for item in sublist]\n",
    "words = flatten\n",
    "wordsunique = set(words)\n",
    "wordsunique = list(wordsunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions for TD-IDF / BM25\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc)\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / (0.01 + n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictonary of words\n",
    "# THIS ONE-TIME INDEXING IS THE MOST PROCESSOR-INTENSIVE STEP AND WILL \n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "plottest = plot_data[0][0:1000]\n",
    "\n",
    "worddic = {}\n",
    "\n",
    "for doc in plottest:\n",
    "    for word in wordsunique:\n",
    "        if word in doc:\n",
    "            word = str(word)\n",
    "            index = plottest.index(doc)\n",
    "            positions = list(np.where(np.array(plottest[index]) == word)[0])\n",
    "            idfs = tfidf(word,doc,plottest)\n",
    "            try:\n",
    "                worddic[word].append([index,positions,idfs])\n",
    "            except:\n",
    "                worddic[word] = []\n",
    "                worddic[word].append([index,positions,idfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-86-40464c3e4aa0>, line 2)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-86-40464c3e4aa0>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    TAKE TIME TO RUN (BUT ONLY NEEDS TO BE RUN ONCE)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# the index creates a dic with each word as a KEY and a list of doc indexs, word positions, and td-idf score as VALUES\n",
    "TAKE TIME TO RUN (BUT ONLY NEEDS TO BE RUN ONCE)\n",
    "worddic['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickel (save) the dictonary to avoid re-calculating\n",
    "np.save('worddic_1000.npy', worddic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import googletrans\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "translations = {}\n",
    "\n",
    "translations['Bonjour'] = translator.translate('bonjour').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Hello'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "translations['Bonjour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}